@page "/speech"
@using CognitiveServices.Explorer.Application.Profiles.Queries
@using CognitiveServices.Explorer.Domain.Speech
@using CognitiveServices.Explorer.Domain.Text
@using CognitiveServices.Explorer.Domain.Profiles
@using CognitiveServices.Explorer.Web.Shared.Profiles
@using MediatR
@inject IJSRuntime JSRuntime
@inject IMediator Mediator

<ProfileSelector OnUpdated="UpdatedProfile" />

<h3>Speech prototype (early beta)</h3>

<p>
    Experimental feature, which hasn't been nicely integrated in client-side Blazor.<br />
    Most of the capture logic is happening on JS side and might not work all the time.
</p>

<ol>
    <li>Enter Speech API details (in Profile top right)</li>
    <li>Click Init recording (once only)</li>
    <li>Start recording</li>
    <li>Stop recording</li>
    <li>Send to Speech API</li>
</ol>

@if (_currentProfile?.SpeechApiConfig?.IsConfigured() != true)
{
    <div class="alert alert-warning" role="alert">Current profile doesn't have Speech configuration.</div>
}

@if (!_isRecordButtonVisible)
{
    <button @onclick="StartRecording">Init recording</button>
}

<button id="recorder-button" style="@(_isRecordButtonVisible ? "" : "display: none;")">
    <div id="recorder-button-div" style="background-color: red; border-radius: 50%; width: 100px; height: 100px;"></div>
</button>
<audio id="audio"></audio>

<button @onclick="DownloadLastRecording">Send to Speech API</button>

<small><pre>DEBUG: @_debugText</pre></small>

@if (_isSpeechSuccsefull)
{
    <p>
        <h4>Recognized text</h4>
        @_recognizedText
    </p>

    <button @onclick="SendToTextApi">Send to Text API</button>

    <pre>@_textAnalysis</pre>
}

<h3>Additional demos</h3>

<ul>
    <li><a href="https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/">Microsoft Cognitive Services Speech API intro demo</a></li>
    <li><a href="https://speech.microsoft.com" target="_blank">Speech Studio by Microsoft</a></li>
    <li><a href="https://github.com/timleyden/speechtotextdemo" target="_blank">Speech to Text Batch Transcription - Demo</a> <small>(no live version)</small></li>
    <li><a href="https://github.com/jernejk/CognitiveServicesDemo.CustomerSupport" target="_blank">Customer Support demo (Console .NET Core)</a> <small>(no live version)</small></li>
</ul>

@code {
    private bool _isRecordButtonVisible = false;
    private string _debugText = string.Empty;
    private string _recognizedText = string.Empty;
    private bool _isSpeechSuccsefull = false;
    private bool _usePreviewVersion = true;
    private string _textAnalysis = string.Empty;
    private Profile? _currentProfile;

    protected async override Task OnInitializedAsync()
    {
        await base.OnInitializedAsync();
        _currentProfile = await Mediator.Send(new GetCurrentProfileQuery());
    }

    public async Task UpdatedProfile()
    {
        //await viewModel.LoadLatestConfig();
        //await viewModel.GetGroups();

        _currentProfile = await Mediator.Send(new GetCurrentProfileQuery());
        base.StateHasChanged();
    }

    public async Task StartRecording()
    {
        _isRecordButtonVisible = true;
        await JSRuntime.InvokeVoidAsync("initAudioRecording");
    }

    public async Task DownloadLastRecording()
    {
        if (_currentProfile?.SpeechApiConfig?.IsConfigured() != true)
        {
            // TODO: Say it needs to be configured.
            return;
        }

        var url = await JSRuntime.InvokeAsync<string>("getLastAudioRecording");

        try
        {
            HttpClient client = new HttpClient();

            var data = await client.GetByteArrayAsync(url);

            _debugText = string.Empty;
            Console.WriteLine("Audio size: " + data.Length);

            client.DefaultRequestHeaders.Add("ContentType", "audio/wav; codecs=audio/pcm; samplerate=16000");
            client.DefaultRequestHeaders.Add("Ocp-Apim-Subscription-Key", _currentProfile.SpeechApiConfig.Token);

            var tokenIssuerBaseUri = new Uri(_currentProfile.SpeechApiConfig.BaseUrl);
            var region = tokenIssuerBaseUri.Host.Substring(0, tokenIssuerBaseUri.Host.IndexOf('.'));

            var uri = new Uri($"https://{region}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US&format=detailed");

            try
            {
                var response = await client.PostAsync(uri, new ByteArrayContent(data));
                Console.WriteLine(response.StatusCode);

                string json = await response.Content.ReadAsStringAsync();
                _debugText = json;

                var speechResult = System.Text.Json.JsonSerializer.Deserialize<SpeechRecognizedResult>(json, new System.Text.Json.JsonSerializerOptions
                {
                    PropertyNameCaseInsensitive = false
                });

                if (speechResult?.NBest?.Any() == true)
                {
                    _recognizedText = speechResult.NBest.First().Display;
                    _isSpeechSuccsefull = true;
                }
                else if (!string.IsNullOrWhiteSpace(speechResult?.RecognitionStatus))
                {
                    _debugText = "Status: " + speechResult!.RecognitionStatus;
                }
            }
            catch (Exception e)
            {
                _debugText = "Failed to get response: " + e.ToString();
                Console.WriteLine("Exception: " + e);
            }

        }
        catch (Exception e)
        {
            _debugText = "Failed: " + e;
        }
    }

    public async Task SendToTextApi()
    {
        if (_currentProfile?.TextApiConfig?.IsConfigured() != true)
        {
            // TODO: Say it needs to be configured.
            _textAnalysis = $"Text API not configured in your Profile.";
            return;
        }

        _textAnalysis = $"Text analysis for `{_recognizedText}`:\n";

        try
        {
            var documentRequest = new TextApiRequest
            {
                documents = new Document[]
                {
                    new Document
                    {
                        language = "en",
                        id = "1",
                        text = _recognizedText
                    }
                }
            };

            AnalysedDocument sentimentResult = await AnalyzeDocument(documentRequest, "sentiment");
            if (sentimentResult != null)
            {
                // We get back score representing sentiment.
                if (_usePreviewVersion)
                {
                    // We are getting a more accurate representation of how positive, negative and neutral the text is.
                    _textAnalysis += $"  Sentiment is {sentimentResult.sentiment} with scores:\n";
                    _textAnalysis += $"   - Positive:  {Math.Round(sentimentResult.confidenceScores.positive * 100, 2)}%\n";
                    _textAnalysis += $"   - Neutral:   {Math.Round(sentimentResult.confidenceScores.neutral * 100, 2)}%\n";
                    _textAnalysis += $"   - Negative:  {Math.Round(sentimentResult.confidenceScores.negative * 100, 2)}%\n";
                }
                else
                {
                    // We only get how potentially positive the text is in Sentiment analysis v2.
                    double score = sentimentResult.score;

                    // Try to determine if message is positive, negative or neutral.
                    string sentiment = score >= 0.75 ? "positive" : (score < 0.25 ? "negative" : "neutral");
                    _textAnalysis += $"  Sentiment is {sentiment} ({Math.Round(score * 100)}%)\n";
                }
            }
            else
            {
                _textAnalysis += "  No sentiment found.\n";
            }


            _textAnalysis += "\n";

            AnalysedDocument keyPhrasesResult = await AnalyzeDocument(documentRequest, "keyPhrases");
            if (keyPhrasesResult?.keyPhrases?.Any() == true)
            {
                _textAnalysis += $"  Key phrases:\n";
                foreach (var keyPhrase in keyPhrasesResult.keyPhrases)
                {
                    _textAnalysis += $"   - {keyPhrase}\n";
                }
            }
            else
            {
                _textAnalysis += "  No key phrases found.\n";
            }

            _textAnalysis += "\n";

            AnalysedDocument namedEntitiesResult = await AnalyzeDocument(documentRequest, "entities");
            if (namedEntitiesResult?.entities?.Any() == true)
            {
                _textAnalysis += "  Entities:\n";
                foreach (var entity in namedEntitiesResult.entities)
                {
                    _textAnalysis += $"   - {entity.name} ({entity.type})\n";
                    if (!string.IsNullOrWhiteSpace(entity.wikipediaUrl))
                    {
                        _textAnalysis += $"       {entity.wikipediaUrl}\n";
                    }
                }
            }
            else
            {
                _textAnalysis += "  No entities found.\n";
            }
        }
        catch (Exception e)
        {
            _debugText = "Failed: " + e;
        }
    }

    private async Task<AnalysedDocument?> AnalyzeDocument(TextApiRequest sentimentDocument, string textAnalysisType)
    {
        // A preview version 3 only exists for sentiment analysis.
        string version = _usePreviewVersion && textAnalysisType == "sentiment" ? "3.0" : "2.1";

        TextApiResponse textApiResponse;
        using (var client = new HttpClient())
        {
            client.DefaultRequestHeaders.Add("Ocp-Apim-Subscription-Key", _currentProfile.TextApiConfig.Token);

            string json = Newtonsoft.Json.JsonConvert.SerializeObject(sentimentDocument);
            var content = new StringContent(json, System.Text.Encoding.UTF8, "application/json");
            var response = await client.PostAsync($"{_currentProfile.TextApiConfig.BaseUrl}/text/analytics/v{version}/{textAnalysisType}", content);

            string responseJson = await response.Content.ReadAsStringAsync();
            textApiResponse = Newtonsoft.Json.JsonConvert.DeserializeObject<TextApiResponse>(responseJson);
        }

        return textApiResponse?.documents?.FirstOrDefault();
    }
}
